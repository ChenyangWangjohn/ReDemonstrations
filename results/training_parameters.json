{
  "checkpoint": "/data/johnwang/ICL/LLaMA-Factory/outputs/qwen3-1.7b-bad-sft-mathinstruct/checkpoint-14740",
  "base_model": "/data/johnwang/huggingface_cache/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e/",
  "dataset": "TIGER-Lab/MathInstruct",
  "training_method": "LoRA (Low-Rank Adaptation)",
  "lora_parameters": {
    "lora_rank": 8,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "lora_target": "all"
  },
  "training_hyperparameters": {
    "per_device_train_batch_size": 8,
    "gradient_accumulation_steps": 2,
    "effective_batch_size": 16,
    "learning_rate": 0.001,
    "num_train_epochs": 1,
    "lr_scheduler_type": "linear",
    "warmup_ratio": 0.05,
    "cutoff_len": 2048,
    "precision": "bf16"
  },
  "output_settings": {
    "logging_steps": 50,
    "save_steps": 1000,
    "plot_loss": true
  },
  "purpose": "Create 'bad SFT' LoRA adapter to corrupt model's commonsense reasoning prior, providing space for ICL correction experiments",
  "training_summary": {
    "global_step": 14740,
    "epoch": 1.0,
    "final_loss": 0.4171,
    "final_learning_rate": 2.927944011997429e-06
  }
}