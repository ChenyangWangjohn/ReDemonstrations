{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 14740,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0033921302578018998,
      "grad_norm": 0.6204606890678406,
      "learning_rate": 6.648575305291724e-05,
      "loss": 1.0169,
      "step": 50
    },
    {
      "epoch": 0.0067842605156037995,
      "grad_norm": 0.7065650820732117,
      "learning_rate": 0.00013432835820895522,
      "loss": 0.5563,
      "step": 100
    },
    {
      "epoch": 0.0101763907734057,
      "grad_norm": 0.4717390537261963,
      "learning_rate": 0.00020217096336499322,
      "loss": 0.5479,
      "step": 150
    },
    {
      "epoch": 0.013568521031207599,
      "grad_norm": 0.5710653066635132,
      "learning_rate": 0.0002700135685210312,
      "loss": 0.5336,
      "step": 200
    },
    {
      "epoch": 0.016960651289009497,
      "grad_norm": 0.34267786145210266,
      "learning_rate": 0.00033785617367706924,
      "loss": 0.479,
      "step": 250
    },
    {
      "epoch": 0.0203527815468114,
      "grad_norm": 0.48124492168426514,
      "learning_rate": 0.0004056987788331072,
      "loss": 0.5107,
      "step": 300
    },
    {
      "epoch": 0.023744911804613297,
      "grad_norm": 0.4362296462059021,
      "learning_rate": 0.0004735413839891452,
      "loss": 0.5102,
      "step": 350
    },
    {
      "epoch": 0.027137042062415198,
      "grad_norm": 0.5253086686134338,
      "learning_rate": 0.0005413839891451832,
      "loss": 0.5322,
      "step": 400
    },
    {
      "epoch": 0.030529172320217096,
      "grad_norm": 0.6320428848266602,
      "learning_rate": 0.0006092265943012212,
      "loss": 0.5259,
      "step": 450
    },
    {
      "epoch": 0.033921302578018994,
      "grad_norm": 0.6600591540336609,
      "learning_rate": 0.0006770691994572592,
      "loss": 0.5414,
      "step": 500
    },
    {
      "epoch": 0.03731343283582089,
      "grad_norm": 0.5305728316307068,
      "learning_rate": 0.0007449118046132972,
      "loss": 0.5293,
      "step": 550
    },
    {
      "epoch": 0.0407055630936228,
      "grad_norm": 0.7738066911697388,
      "learning_rate": 0.0008127544097693352,
      "loss": 0.541,
      "step": 600
    },
    {
      "epoch": 0.044097693351424695,
      "grad_norm": 0.7197979688644409,
      "learning_rate": 0.0008805970149253731,
      "loss": 0.5478,
      "step": 650
    },
    {
      "epoch": 0.04748982360922659,
      "grad_norm": 0.8210131525993347,
      "learning_rate": 0.0009484396200814112,
      "loss": 0.5693,
      "step": 700
    },
    {
      "epoch": 0.05088195386702849,
      "grad_norm": 0.9766448140144348,
      "learning_rate": 0.0009991430407769763,
      "loss": 0.5852,
      "step": 750
    },
    {
      "epoch": 0.054274084124830396,
      "grad_norm": 1.155045509338379,
      "learning_rate": 0.0009955723773477111,
      "loss": 0.6068,
      "step": 800
    },
    {
      "epoch": 0.057666214382632294,
      "grad_norm": 1.0651930570602417,
      "learning_rate": 0.000992001713918446,
      "loss": 0.6184,
      "step": 850
    },
    {
      "epoch": 0.06105834464043419,
      "grad_norm": 0.8935092687606812,
      "learning_rate": 0.0009884310504891809,
      "loss": 0.6369,
      "step": 900
    },
    {
      "epoch": 0.06445047489823609,
      "grad_norm": 1.2148183584213257,
      "learning_rate": 0.0009848603870599157,
      "loss": 0.64,
      "step": 950
    },
    {
      "epoch": 0.06784260515603799,
      "grad_norm": 1.0266457796096802,
      "learning_rate": 0.0009812897236306506,
      "loss": 0.6369,
      "step": 1000
    },
    {
      "epoch": 0.07123473541383989,
      "grad_norm": 1.1196649074554443,
      "learning_rate": 0.0009777190602013855,
      "loss": 0.6437,
      "step": 1050
    },
    {
      "epoch": 0.07462686567164178,
      "grad_norm": 0.9611613154411316,
      "learning_rate": 0.0009741483967721204,
      "loss": 0.6353,
      "step": 1100
    },
    {
      "epoch": 0.0780189959294437,
      "grad_norm": 0.7836260199546814,
      "learning_rate": 0.0009705777333428551,
      "loss": 0.6458,
      "step": 1150
    },
    {
      "epoch": 0.0814111261872456,
      "grad_norm": 1.0628358125686646,
      "learning_rate": 0.00096700706991359,
      "loss": 0.6292,
      "step": 1200
    },
    {
      "epoch": 0.08480325644504749,
      "grad_norm": 1.0184965133666992,
      "learning_rate": 0.0009634364064843248,
      "loss": 0.6331,
      "step": 1250
    },
    {
      "epoch": 0.08819538670284939,
      "grad_norm": 1.2384834289550781,
      "learning_rate": 0.0009598657430550597,
      "loss": 0.6388,
      "step": 1300
    },
    {
      "epoch": 0.09158751696065129,
      "grad_norm": 0.9144036769866943,
      "learning_rate": 0.0009562950796257946,
      "loss": 0.6281,
      "step": 1350
    },
    {
      "epoch": 0.09497964721845319,
      "grad_norm": 1.5526167154312134,
      "learning_rate": 0.0009527244161965293,
      "loss": 0.6111,
      "step": 1400
    },
    {
      "epoch": 0.09837177747625508,
      "grad_norm": 1.1938942670822144,
      "learning_rate": 0.0009491537527672641,
      "loss": 0.6474,
      "step": 1450
    },
    {
      "epoch": 0.10176390773405698,
      "grad_norm": 0.9410785436630249,
      "learning_rate": 0.000945583089337999,
      "loss": 0.6159,
      "step": 1500
    },
    {
      "epoch": 0.10515603799185888,
      "grad_norm": 1.406425952911377,
      "learning_rate": 0.0009420124259087338,
      "loss": 0.6301,
      "step": 1550
    },
    {
      "epoch": 0.10854816824966079,
      "grad_norm": 1.2019864320755005,
      "learning_rate": 0.0009384417624794687,
      "loss": 0.6161,
      "step": 1600
    },
    {
      "epoch": 0.11194029850746269,
      "grad_norm": 1.0306479930877686,
      "learning_rate": 0.0009348710990502035,
      "loss": 0.6147,
      "step": 1650
    },
    {
      "epoch": 0.11533242876526459,
      "grad_norm": 1.2987719774246216,
      "learning_rate": 0.0009313004356209383,
      "loss": 0.6006,
      "step": 1700
    },
    {
      "epoch": 0.11872455902306649,
      "grad_norm": 0.9113412499427795,
      "learning_rate": 0.0009277297721916732,
      "loss": 0.5989,
      "step": 1750
    },
    {
      "epoch": 0.12211668928086838,
      "grad_norm": 1.540543556213379,
      "learning_rate": 0.0009241591087624081,
      "loss": 0.6098,
      "step": 1800
    },
    {
      "epoch": 0.1255088195386703,
      "grad_norm": 1.1848560571670532,
      "learning_rate": 0.0009205884453331429,
      "loss": 0.6119,
      "step": 1850
    },
    {
      "epoch": 0.12890094979647218,
      "grad_norm": 1.2500065565109253,
      "learning_rate": 0.0009170177819038777,
      "loss": 0.6205,
      "step": 1900
    },
    {
      "epoch": 0.1322930800542741,
      "grad_norm": 1.4667108058929443,
      "learning_rate": 0.0009134471184746126,
      "loss": 0.6366,
      "step": 1950
    },
    {
      "epoch": 0.13568521031207598,
      "grad_norm": 1.0548937320709229,
      "learning_rate": 0.0009098764550453474,
      "loss": 0.5736,
      "step": 2000
    },
    {
      "epoch": 0.1390773405698779,
      "grad_norm": 0.8228588104248047,
      "learning_rate": 0.0009063057916160823,
      "loss": 0.6096,
      "step": 2050
    },
    {
      "epoch": 0.14246947082767977,
      "grad_norm": 1.0601565837860107,
      "learning_rate": 0.0009027351281868172,
      "loss": 0.632,
      "step": 2100
    },
    {
      "epoch": 0.14586160108548168,
      "grad_norm": 0.8116012811660767,
      "learning_rate": 0.0008991644647575519,
      "loss": 0.5849,
      "step": 2150
    },
    {
      "epoch": 0.14925373134328357,
      "grad_norm": 1.4843378067016602,
      "learning_rate": 0.0008955938013282868,
      "loss": 0.6149,
      "step": 2200
    },
    {
      "epoch": 0.15264586160108548,
      "grad_norm": 1.0682169198989868,
      "learning_rate": 0.0008920231378990217,
      "loss": 0.6148,
      "step": 2250
    },
    {
      "epoch": 0.1560379918588874,
      "grad_norm": 0.8533715009689331,
      "learning_rate": 0.0008884524744697565,
      "loss": 0.5888,
      "step": 2300
    },
    {
      "epoch": 0.15943012211668928,
      "grad_norm": 0.957858681678772,
      "learning_rate": 0.0008848818110404914,
      "loss": 0.6341,
      "step": 2350
    },
    {
      "epoch": 0.1628222523744912,
      "grad_norm": 0.9158526062965393,
      "learning_rate": 0.0008813111476112263,
      "loss": 0.6063,
      "step": 2400
    },
    {
      "epoch": 0.16621438263229307,
      "grad_norm": 0.8975407481193542,
      "learning_rate": 0.000877740484181961,
      "loss": 0.5812,
      "step": 2450
    },
    {
      "epoch": 0.16960651289009498,
      "grad_norm": 1.072521448135376,
      "learning_rate": 0.0008741698207526959,
      "loss": 0.6026,
      "step": 2500
    },
    {
      "epoch": 0.17299864314789687,
      "grad_norm": 1.228833556175232,
      "learning_rate": 0.0008705991573234308,
      "loss": 0.5979,
      "step": 2550
    },
    {
      "epoch": 0.17639077340569878,
      "grad_norm": 1.3429028987884521,
      "learning_rate": 0.0008670284938941656,
      "loss": 0.5772,
      "step": 2600
    },
    {
      "epoch": 0.17978290366350066,
      "grad_norm": 1.1637122631072998,
      "learning_rate": 0.0008634578304649005,
      "loss": 0.5781,
      "step": 2650
    },
    {
      "epoch": 0.18317503392130258,
      "grad_norm": 1.176674723625183,
      "learning_rate": 0.0008598871670356351,
      "loss": 0.5889,
      "step": 2700
    },
    {
      "epoch": 0.1865671641791045,
      "grad_norm": 1.012502908706665,
      "learning_rate": 0.00085631650360637,
      "loss": 0.6013,
      "step": 2750
    },
    {
      "epoch": 0.18995929443690637,
      "grad_norm": 0.9488440155982971,
      "learning_rate": 0.0008527458401771049,
      "loss": 0.6027,
      "step": 2800
    },
    {
      "epoch": 0.19335142469470828,
      "grad_norm": 0.936941385269165,
      "learning_rate": 0.0008491751767478398,
      "loss": 0.5964,
      "step": 2850
    },
    {
      "epoch": 0.19674355495251017,
      "grad_norm": 1.061122179031372,
      "learning_rate": 0.0008456045133185746,
      "loss": 0.6001,
      "step": 2900
    },
    {
      "epoch": 0.20013568521031208,
      "grad_norm": 1.1259897947311401,
      "learning_rate": 0.0008420338498893094,
      "loss": 0.5945,
      "step": 2950
    },
    {
      "epoch": 0.20352781546811397,
      "grad_norm": 1.3873032331466675,
      "learning_rate": 0.0008384631864600442,
      "loss": 0.5888,
      "step": 3000
    },
    {
      "epoch": 0.20691994572591588,
      "grad_norm": 1.4632649421691895,
      "learning_rate": 0.0008348925230307791,
      "loss": 0.5992,
      "step": 3050
    },
    {
      "epoch": 0.21031207598371776,
      "grad_norm": 1.3712574243545532,
      "learning_rate": 0.000831321859601514,
      "loss": 0.5845,
      "step": 3100
    },
    {
      "epoch": 0.21370420624151967,
      "grad_norm": 0.9193218350410461,
      "learning_rate": 0.0008277511961722489,
      "loss": 0.6014,
      "step": 3150
    },
    {
      "epoch": 0.21709633649932158,
      "grad_norm": 1.245445728302002,
      "learning_rate": 0.0008241805327429836,
      "loss": 0.6157,
      "step": 3200
    },
    {
      "epoch": 0.22048846675712347,
      "grad_norm": 1.4740654230117798,
      "learning_rate": 0.0008206098693137185,
      "loss": 0.5998,
      "step": 3250
    },
    {
      "epoch": 0.22388059701492538,
      "grad_norm": 1.0316526889801025,
      "learning_rate": 0.0008170392058844533,
      "loss": 0.5687,
      "step": 3300
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 0.9196860790252686,
      "learning_rate": 0.0008134685424551882,
      "loss": 0.587,
      "step": 3350
    },
    {
      "epoch": 0.23066485753052918,
      "grad_norm": 0.9940120577812195,
      "learning_rate": 0.0008098978790259231,
      "loss": 0.6051,
      "step": 3400
    },
    {
      "epoch": 0.23405698778833106,
      "grad_norm": 1.2595832347869873,
      "learning_rate": 0.0008063272155966578,
      "loss": 0.5947,
      "step": 3450
    },
    {
      "epoch": 0.23744911804613297,
      "grad_norm": 0.8397995233535767,
      "learning_rate": 0.0008027565521673927,
      "loss": 0.631,
      "step": 3500
    },
    {
      "epoch": 0.24084124830393486,
      "grad_norm": 1.177376627922058,
      "learning_rate": 0.0007991858887381276,
      "loss": 0.5989,
      "step": 3550
    },
    {
      "epoch": 0.24423337856173677,
      "grad_norm": 1.1976821422576904,
      "learning_rate": 0.0007956152253088624,
      "loss": 0.5716,
      "step": 3600
    },
    {
      "epoch": 0.24762550881953868,
      "grad_norm": 1.057816505432129,
      "learning_rate": 0.0007920445618795973,
      "loss": 0.5597,
      "step": 3650
    },
    {
      "epoch": 0.2510176390773406,
      "grad_norm": 1.2329250574111938,
      "learning_rate": 0.0007884738984503321,
      "loss": 0.5794,
      "step": 3700
    },
    {
      "epoch": 0.2544097693351425,
      "grad_norm": 1.1374918222427368,
      "learning_rate": 0.0007849032350210669,
      "loss": 0.6031,
      "step": 3750
    },
    {
      "epoch": 0.25780189959294436,
      "grad_norm": 1.0030301809310913,
      "learning_rate": 0.0007813325715918018,
      "loss": 0.5942,
      "step": 3800
    },
    {
      "epoch": 0.26119402985074625,
      "grad_norm": 1.0195225477218628,
      "learning_rate": 0.0007777619081625367,
      "loss": 0.5644,
      "step": 3850
    },
    {
      "epoch": 0.2645861601085482,
      "grad_norm": 1.0096404552459717,
      "learning_rate": 0.0007741912447332714,
      "loss": 0.5849,
      "step": 3900
    },
    {
      "epoch": 0.26797829036635007,
      "grad_norm": 1.0862622261047363,
      "learning_rate": 0.0007706205813040063,
      "loss": 0.5759,
      "step": 3950
    },
    {
      "epoch": 0.27137042062415195,
      "grad_norm": 1.3188953399658203,
      "learning_rate": 0.0007670499178747411,
      "loss": 0.5782,
      "step": 4000
    },
    {
      "epoch": 0.2747625508819539,
      "grad_norm": 1.098307728767395,
      "learning_rate": 0.0007634792544454759,
      "loss": 0.5728,
      "step": 4050
    },
    {
      "epoch": 0.2781546811397558,
      "grad_norm": 1.0693916082382202,
      "learning_rate": 0.0007599085910162108,
      "loss": 0.5736,
      "step": 4100
    },
    {
      "epoch": 0.28154681139755766,
      "grad_norm": 1.1038962602615356,
      "learning_rate": 0.0007563379275869457,
      "loss": 0.5703,
      "step": 4150
    },
    {
      "epoch": 0.28493894165535955,
      "grad_norm": 2.0083680152893066,
      "learning_rate": 0.0007527672641576805,
      "loss": 0.5782,
      "step": 4200
    },
    {
      "epoch": 0.2883310719131615,
      "grad_norm": 1.3735461235046387,
      "learning_rate": 0.0007491966007284153,
      "loss": 0.5848,
      "step": 4250
    },
    {
      "epoch": 0.29172320217096337,
      "grad_norm": 1.2157557010650635,
      "learning_rate": 0.0007456259372991502,
      "loss": 0.5743,
      "step": 4300
    },
    {
      "epoch": 0.29511533242876525,
      "grad_norm": 1.1317594051361084,
      "learning_rate": 0.000742055273869885,
      "loss": 0.5571,
      "step": 4350
    },
    {
      "epoch": 0.29850746268656714,
      "grad_norm": 1.5682648420333862,
      "learning_rate": 0.0007384846104406199,
      "loss": 0.575,
      "step": 4400
    },
    {
      "epoch": 0.3018995929443691,
      "grad_norm": 0.9259466528892517,
      "learning_rate": 0.0007349139470113548,
      "loss": 0.5535,
      "step": 4450
    },
    {
      "epoch": 0.30529172320217096,
      "grad_norm": 1.2449257373809814,
      "learning_rate": 0.0007313432835820895,
      "loss": 0.5789,
      "step": 4500
    },
    {
      "epoch": 0.30868385345997285,
      "grad_norm": 1.2738820314407349,
      "learning_rate": 0.0007277726201528244,
      "loss": 0.554,
      "step": 4550
    },
    {
      "epoch": 0.3120759837177748,
      "grad_norm": 0.8863123059272766,
      "learning_rate": 0.0007242019567235593,
      "loss": 0.5663,
      "step": 4600
    },
    {
      "epoch": 0.31546811397557667,
      "grad_norm": 1.0680416822433472,
      "learning_rate": 0.0007206312932942941,
      "loss": 0.5732,
      "step": 4650
    },
    {
      "epoch": 0.31886024423337855,
      "grad_norm": 1.188347578048706,
      "learning_rate": 0.000717060629865029,
      "loss": 0.5522,
      "step": 4700
    },
    {
      "epoch": 0.32225237449118044,
      "grad_norm": 0.9578662514686584,
      "learning_rate": 0.0007134899664357638,
      "loss": 0.5612,
      "step": 4750
    },
    {
      "epoch": 0.3256445047489824,
      "grad_norm": 0.9288416504859924,
      "learning_rate": 0.0007099193030064986,
      "loss": 0.563,
      "step": 4800
    },
    {
      "epoch": 0.32903663500678426,
      "grad_norm": 1.0315266847610474,
      "learning_rate": 0.0007063486395772335,
      "loss": 0.5587,
      "step": 4850
    },
    {
      "epoch": 0.33242876526458615,
      "grad_norm": 0.7981466054916382,
      "learning_rate": 0.0007027779761479684,
      "loss": 0.5534,
      "step": 4900
    },
    {
      "epoch": 0.3358208955223881,
      "grad_norm": 1.19997239112854,
      "learning_rate": 0.0006992073127187032,
      "loss": 0.5763,
      "step": 4950
    },
    {
      "epoch": 0.33921302578018997,
      "grad_norm": 0.8310846090316772,
      "learning_rate": 0.000695636649289438,
      "loss": 0.542,
      "step": 5000
    },
    {
      "epoch": 0.34260515603799185,
      "grad_norm": 1.000855565071106,
      "learning_rate": 0.0006920659858601729,
      "loss": 0.5532,
      "step": 5050
    },
    {
      "epoch": 0.34599728629579374,
      "grad_norm": 0.8179265260696411,
      "learning_rate": 0.0006884953224309076,
      "loss": 0.5556,
      "step": 5100
    },
    {
      "epoch": 0.3493894165535957,
      "grad_norm": 0.9657702445983887,
      "learning_rate": 0.0006849246590016425,
      "loss": 0.5342,
      "step": 5150
    },
    {
      "epoch": 0.35278154681139756,
      "grad_norm": 1.0045948028564453,
      "learning_rate": 0.0006813539955723774,
      "loss": 0.5507,
      "step": 5200
    },
    {
      "epoch": 0.35617367706919945,
      "grad_norm": 1.1450870037078857,
      "learning_rate": 0.0006777833321431121,
      "loss": 0.5246,
      "step": 5250
    },
    {
      "epoch": 0.35956580732700133,
      "grad_norm": 0.8702640533447266,
      "learning_rate": 0.000674212668713847,
      "loss": 0.5559,
      "step": 5300
    },
    {
      "epoch": 0.36295793758480327,
      "grad_norm": 0.9237738847732544,
      "learning_rate": 0.0006706420052845818,
      "loss": 0.5688,
      "step": 5350
    },
    {
      "epoch": 0.36635006784260515,
      "grad_norm": 1.4333951473236084,
      "learning_rate": 0.0006670713418553167,
      "loss": 0.5561,
      "step": 5400
    },
    {
      "epoch": 0.36974219810040704,
      "grad_norm": 1.3160306215286255,
      "learning_rate": 0.0006635006784260516,
      "loss": 0.5463,
      "step": 5450
    },
    {
      "epoch": 0.373134328358209,
      "grad_norm": 1.1228227615356445,
      "learning_rate": 0.0006599300149967865,
      "loss": 0.575,
      "step": 5500
    },
    {
      "epoch": 0.37652645861601086,
      "grad_norm": 1.1436853408813477,
      "learning_rate": 0.0006563593515675212,
      "loss": 0.5417,
      "step": 5550
    },
    {
      "epoch": 0.37991858887381275,
      "grad_norm": 1.5514202117919922,
      "learning_rate": 0.0006527886881382561,
      "loss": 0.5404,
      "step": 5600
    },
    {
      "epoch": 0.38331071913161463,
      "grad_norm": 0.9098592400550842,
      "learning_rate": 0.000649218024708991,
      "loss": 0.5804,
      "step": 5650
    },
    {
      "epoch": 0.38670284938941657,
      "grad_norm": 0.8180974721908569,
      "learning_rate": 0.0006456473612797258,
      "loss": 0.5537,
      "step": 5700
    },
    {
      "epoch": 0.39009497964721845,
      "grad_norm": 1.1024178266525269,
      "learning_rate": 0.0006420766978504607,
      "loss": 0.5553,
      "step": 5750
    },
    {
      "epoch": 0.39348710990502034,
      "grad_norm": 0.8611081838607788,
      "learning_rate": 0.0006385060344211954,
      "loss": 0.5644,
      "step": 5800
    },
    {
      "epoch": 0.3968792401628223,
      "grad_norm": 1.124893069267273,
      "learning_rate": 0.0006349353709919303,
      "loss": 0.5531,
      "step": 5850
    },
    {
      "epoch": 0.40027137042062416,
      "grad_norm": 1.1327918767929077,
      "learning_rate": 0.0006313647075626652,
      "loss": 0.5664,
      "step": 5900
    },
    {
      "epoch": 0.40366350067842605,
      "grad_norm": 0.6435866355895996,
      "learning_rate": 0.0006277940441334,
      "loss": 0.5227,
      "step": 5950
    },
    {
      "epoch": 0.40705563093622793,
      "grad_norm": 1.2048728466033936,
      "learning_rate": 0.0006242233807041349,
      "loss": 0.5035,
      "step": 6000
    },
    {
      "epoch": 0.41044776119402987,
      "grad_norm": 0.9442483186721802,
      "learning_rate": 0.0006206527172748697,
      "loss": 0.5248,
      "step": 6050
    },
    {
      "epoch": 0.41383989145183175,
      "grad_norm": 1.1842767000198364,
      "learning_rate": 0.0006170820538456045,
      "loss": 0.5282,
      "step": 6100
    },
    {
      "epoch": 0.41723202170963364,
      "grad_norm": 0.7887049913406372,
      "learning_rate": 0.0006135113904163394,
      "loss": 0.5528,
      "step": 6150
    },
    {
      "epoch": 0.4206241519674355,
      "grad_norm": 0.9299411773681641,
      "learning_rate": 0.0006099407269870743,
      "loss": 0.5268,
      "step": 6200
    },
    {
      "epoch": 0.42401628222523746,
      "grad_norm": 1.1230508089065552,
      "learning_rate": 0.0006063700635578091,
      "loss": 0.5715,
      "step": 6250
    },
    {
      "epoch": 0.42740841248303935,
      "grad_norm": 1.1484003067016602,
      "learning_rate": 0.0006027994001285439,
      "loss": 0.5597,
      "step": 6300
    },
    {
      "epoch": 0.43080054274084123,
      "grad_norm": 1.0103507041931152,
      "learning_rate": 0.0005992287366992787,
      "loss": 0.5422,
      "step": 6350
    },
    {
      "epoch": 0.43419267299864317,
      "grad_norm": 1.0092636346817017,
      "learning_rate": 0.0005956580732700135,
      "loss": 0.5211,
      "step": 6400
    },
    {
      "epoch": 0.43758480325644505,
      "grad_norm": 0.7313674092292786,
      "learning_rate": 0.0005920874098407484,
      "loss": 0.5257,
      "step": 6450
    },
    {
      "epoch": 0.44097693351424694,
      "grad_norm": 0.9933989644050598,
      "learning_rate": 0.0005885167464114833,
      "loss": 0.5348,
      "step": 6500
    },
    {
      "epoch": 0.4443690637720488,
      "grad_norm": 1.2675262689590454,
      "learning_rate": 0.000584946082982218,
      "loss": 0.5492,
      "step": 6550
    },
    {
      "epoch": 0.44776119402985076,
      "grad_norm": 0.871351420879364,
      "learning_rate": 0.0005813754195529529,
      "loss": 0.5011,
      "step": 6600
    },
    {
      "epoch": 0.45115332428765265,
      "grad_norm": 0.9903978705406189,
      "learning_rate": 0.0005778047561236878,
      "loss": 0.5199,
      "step": 6650
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.7917478680610657,
      "learning_rate": 0.0005742340926944226,
      "loss": 0.4959,
      "step": 6700
    },
    {
      "epoch": 0.45793758480325647,
      "grad_norm": 1.1926175355911255,
      "learning_rate": 0.0005706634292651575,
      "loss": 0.5395,
      "step": 6750
    },
    {
      "epoch": 0.46132971506105835,
      "grad_norm": 1.053073525428772,
      "learning_rate": 0.0005670927658358923,
      "loss": 0.5091,
      "step": 6800
    },
    {
      "epoch": 0.46472184531886024,
      "grad_norm": 1.2020068168640137,
      "learning_rate": 0.0005635221024066271,
      "loss": 0.542,
      "step": 6850
    },
    {
      "epoch": 0.4681139755766621,
      "grad_norm": 1.092653512954712,
      "learning_rate": 0.000559951438977362,
      "loss": 0.5165,
      "step": 6900
    },
    {
      "epoch": 0.47150610583446406,
      "grad_norm": 0.8119292855262756,
      "learning_rate": 0.0005563807755480969,
      "loss": 0.5171,
      "step": 6950
    },
    {
      "epoch": 0.47489823609226595,
      "grad_norm": 1.1876264810562134,
      "learning_rate": 0.0005528101121188317,
      "loss": 0.5548,
      "step": 7000
    },
    {
      "epoch": 0.47829036635006783,
      "grad_norm": 0.7664388418197632,
      "learning_rate": 0.0005492394486895666,
      "loss": 0.5077,
      "step": 7050
    },
    {
      "epoch": 0.4816824966078697,
      "grad_norm": 1.0276410579681396,
      "learning_rate": 0.0005456687852603014,
      "loss": 0.5243,
      "step": 7100
    },
    {
      "epoch": 0.48507462686567165,
      "grad_norm": 0.9132844805717468,
      "learning_rate": 0.0005420981218310362,
      "loss": 0.5355,
      "step": 7150
    },
    {
      "epoch": 0.48846675712347354,
      "grad_norm": 0.8479125499725342,
      "learning_rate": 0.0005385274584017711,
      "loss": 0.5224,
      "step": 7200
    },
    {
      "epoch": 0.4918588873812754,
      "grad_norm": 0.9799138307571411,
      "learning_rate": 0.000534956794972506,
      "loss": 0.531,
      "step": 7250
    },
    {
      "epoch": 0.49525101763907736,
      "grad_norm": 0.8465747237205505,
      "learning_rate": 0.0005313861315432408,
      "loss": 0.5122,
      "step": 7300
    },
    {
      "epoch": 0.49864314789687925,
      "grad_norm": 1.0250372886657715,
      "learning_rate": 0.0005278154681139756,
      "loss": 0.54,
      "step": 7350
    },
    {
      "epoch": 0.5020352781546812,
      "grad_norm": 1.3269424438476562,
      "learning_rate": 0.0005242448046847105,
      "loss": 0.5008,
      "step": 7400
    },
    {
      "epoch": 0.505427408412483,
      "grad_norm": 0.7390570044517517,
      "learning_rate": 0.0005206741412554453,
      "loss": 0.5231,
      "step": 7450
    },
    {
      "epoch": 0.508819538670285,
      "grad_norm": 0.961456835269928,
      "learning_rate": 0.0005171034778261802,
      "loss": 0.5152,
      "step": 7500
    },
    {
      "epoch": 0.5122116689280869,
      "grad_norm": 0.9011660218238831,
      "learning_rate": 0.000513532814396915,
      "loss": 0.4889,
      "step": 7550
    },
    {
      "epoch": 0.5156037991858887,
      "grad_norm": 1.3136394023895264,
      "learning_rate": 0.0005099621509676497,
      "loss": 0.5273,
      "step": 7600
    },
    {
      "epoch": 0.5189959294436907,
      "grad_norm": 0.8087366223335266,
      "learning_rate": 0.0005063914875383846,
      "loss": 0.5366,
      "step": 7650
    },
    {
      "epoch": 0.5223880597014925,
      "grad_norm": 0.9786328077316284,
      "learning_rate": 0.0005028208241091194,
      "loss": 0.4983,
      "step": 7700
    },
    {
      "epoch": 0.5257801899592944,
      "grad_norm": 0.8156945109367371,
      "learning_rate": 0.0004992501606798543,
      "loss": 0.5082,
      "step": 7750
    },
    {
      "epoch": 0.5291723202170964,
      "grad_norm": 0.9703748822212219,
      "learning_rate": 0.0004956794972505892,
      "loss": 0.5162,
      "step": 7800
    },
    {
      "epoch": 0.5325644504748982,
      "grad_norm": 1.0767749547958374,
      "learning_rate": 0.0004921088338213239,
      "loss": 0.531,
      "step": 7850
    },
    {
      "epoch": 0.5359565807327001,
      "grad_norm": 0.7223791480064392,
      "learning_rate": 0.0004885381703920588,
      "loss": 0.5149,
      "step": 7900
    },
    {
      "epoch": 0.5393487109905021,
      "grad_norm": 1.2743884325027466,
      "learning_rate": 0.0004849675069627937,
      "loss": 0.5282,
      "step": 7950
    },
    {
      "epoch": 0.5427408412483039,
      "grad_norm": 0.9413390159606934,
      "learning_rate": 0.00048139684353352855,
      "loss": 0.4911,
      "step": 8000
    },
    {
      "epoch": 0.5461329715061058,
      "grad_norm": 1.087246060371399,
      "learning_rate": 0.00047782618010426336,
      "loss": 0.5077,
      "step": 8050
    },
    {
      "epoch": 0.5495251017639078,
      "grad_norm": 0.8215420842170715,
      "learning_rate": 0.00047425551667499823,
      "loss": 0.5378,
      "step": 8100
    },
    {
      "epoch": 0.5529172320217096,
      "grad_norm": 0.968760073184967,
      "learning_rate": 0.0004706848532457331,
      "loss": 0.5027,
      "step": 8150
    },
    {
      "epoch": 0.5563093622795116,
      "grad_norm": 0.8586920499801636,
      "learning_rate": 0.0004671141898164679,
      "loss": 0.5049,
      "step": 8200
    },
    {
      "epoch": 0.5597014925373134,
      "grad_norm": 0.840502142906189,
      "learning_rate": 0.0004635435263872028,
      "loss": 0.5141,
      "step": 8250
    },
    {
      "epoch": 0.5630936227951153,
      "grad_norm": 0.8709524869918823,
      "learning_rate": 0.0004599728629579376,
      "loss": 0.5032,
      "step": 8300
    },
    {
      "epoch": 0.5664857530529173,
      "grad_norm": 0.8945835828781128,
      "learning_rate": 0.00045640219952867246,
      "loss": 0.4933,
      "step": 8350
    },
    {
      "epoch": 0.5698778833107191,
      "grad_norm": 1.131867527961731,
      "learning_rate": 0.00045283153609940733,
      "loss": 0.5226,
      "step": 8400
    },
    {
      "epoch": 0.573270013568521,
      "grad_norm": 0.7160534262657166,
      "learning_rate": 0.0004492608726701421,
      "loss": 0.4828,
      "step": 8450
    },
    {
      "epoch": 0.576662143826323,
      "grad_norm": 0.9774195551872253,
      "learning_rate": 0.00044569020924087695,
      "loss": 0.4819,
      "step": 8500
    },
    {
      "epoch": 0.5800542740841248,
      "grad_norm": 0.8166798949241638,
      "learning_rate": 0.00044211954581161177,
      "loss": 0.4929,
      "step": 8550
    },
    {
      "epoch": 0.5834464043419267,
      "grad_norm": 0.8230364918708801,
      "learning_rate": 0.00043854888238234664,
      "loss": 0.4947,
      "step": 8600
    },
    {
      "epoch": 0.5868385345997287,
      "grad_norm": 0.7964144349098206,
      "learning_rate": 0.0004349782189530815,
      "loss": 0.4927,
      "step": 8650
    },
    {
      "epoch": 0.5902306648575305,
      "grad_norm": 1.086275577545166,
      "learning_rate": 0.0004314075555238163,
      "loss": 0.4896,
      "step": 8700
    },
    {
      "epoch": 0.5936227951153324,
      "grad_norm": 0.9934229254722595,
      "learning_rate": 0.0004278368920945512,
      "loss": 0.502,
      "step": 8750
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 0.9392313361167908,
      "learning_rate": 0.000424266228665286,
      "loss": 0.488,
      "step": 8800
    },
    {
      "epoch": 0.6004070556309362,
      "grad_norm": 1.0533201694488525,
      "learning_rate": 0.00042069556523602087,
      "loss": 0.4795,
      "step": 8850
    },
    {
      "epoch": 0.6037991858887382,
      "grad_norm": 0.9529405832290649,
      "learning_rate": 0.00041712490180675574,
      "loss": 0.4906,
      "step": 8900
    },
    {
      "epoch": 0.60719131614654,
      "grad_norm": 0.7888661026954651,
      "learning_rate": 0.00041355423837749055,
      "loss": 0.4882,
      "step": 8950
    },
    {
      "epoch": 0.6105834464043419,
      "grad_norm": 0.7507513761520386,
      "learning_rate": 0.0004099835749482254,
      "loss": 0.4762,
      "step": 9000
    },
    {
      "epoch": 0.6139755766621439,
      "grad_norm": 1.1059447526931763,
      "learning_rate": 0.00040641291151896023,
      "loss": 0.4961,
      "step": 9050
    },
    {
      "epoch": 0.6173677069199457,
      "grad_norm": 1.025907278060913,
      "learning_rate": 0.00040284224808969504,
      "loss": 0.4827,
      "step": 9100
    },
    {
      "epoch": 0.6207598371777476,
      "grad_norm": 0.8062466979026794,
      "learning_rate": 0.0003992715846604299,
      "loss": 0.4844,
      "step": 9150
    },
    {
      "epoch": 0.6241519674355496,
      "grad_norm": 0.8933464884757996,
      "learning_rate": 0.0003957009212311647,
      "loss": 0.4971,
      "step": 9200
    },
    {
      "epoch": 0.6275440976933514,
      "grad_norm": 1.1838895082473755,
      "learning_rate": 0.0003921302578018996,
      "loss": 0.4783,
      "step": 9250
    },
    {
      "epoch": 0.6309362279511533,
      "grad_norm": 0.7117648720741272,
      "learning_rate": 0.00038855959437263446,
      "loss": 0.4778,
      "step": 9300
    },
    {
      "epoch": 0.6343283582089553,
      "grad_norm": 0.793130099773407,
      "learning_rate": 0.0003849889309433693,
      "loss": 0.4997,
      "step": 9350
    },
    {
      "epoch": 0.6377204884667571,
      "grad_norm": 0.7338709831237793,
      "learning_rate": 0.00038141826751410414,
      "loss": 0.4767,
      "step": 9400
    },
    {
      "epoch": 0.641112618724559,
      "grad_norm": 0.9482817053794861,
      "learning_rate": 0.00037784760408483896,
      "loss": 0.4674,
      "step": 9450
    },
    {
      "epoch": 0.6445047489823609,
      "grad_norm": 0.9298919439315796,
      "learning_rate": 0.0003742769406555738,
      "loss": 0.5106,
      "step": 9500
    },
    {
      "epoch": 0.6478968792401628,
      "grad_norm": 0.7375963926315308,
      "learning_rate": 0.0003707062772263087,
      "loss": 0.4771,
      "step": 9550
    },
    {
      "epoch": 0.6512890094979648,
      "grad_norm": 0.8638957142829895,
      "learning_rate": 0.0003671356137970435,
      "loss": 0.4663,
      "step": 9600
    },
    {
      "epoch": 0.6546811397557666,
      "grad_norm": 0.9399363398551941,
      "learning_rate": 0.0003635649503677783,
      "loss": 0.4854,
      "step": 9650
    },
    {
      "epoch": 0.6580732700135685,
      "grad_norm": 0.9606402516365051,
      "learning_rate": 0.00035999428693851313,
      "loss": 0.4848,
      "step": 9700
    },
    {
      "epoch": 0.6614654002713705,
      "grad_norm": 0.8798911571502686,
      "learning_rate": 0.000356423623509248,
      "loss": 0.4701,
      "step": 9750
    },
    {
      "epoch": 0.6648575305291723,
      "grad_norm": 0.6553074717521667,
      "learning_rate": 0.00035285296007998287,
      "loss": 0.4898,
      "step": 9800
    },
    {
      "epoch": 0.6682496607869742,
      "grad_norm": 1.099178671836853,
      "learning_rate": 0.0003492822966507177,
      "loss": 0.4566,
      "step": 9850
    },
    {
      "epoch": 0.6716417910447762,
      "grad_norm": 0.8603426814079285,
      "learning_rate": 0.00034571163322145255,
      "loss": 0.4802,
      "step": 9900
    },
    {
      "epoch": 0.675033921302578,
      "grad_norm": 0.8849459290504456,
      "learning_rate": 0.0003421409697921874,
      "loss": 0.475,
      "step": 9950
    },
    {
      "epoch": 0.6784260515603799,
      "grad_norm": 0.7985400557518005,
      "learning_rate": 0.00033857030636292223,
      "loss": 0.4782,
      "step": 10000
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 1.4004842042922974,
      "learning_rate": 0.0003349996429336571,
      "loss": 0.4949,
      "step": 10050
    },
    {
      "epoch": 0.6852103120759837,
      "grad_norm": 0.8742559552192688,
      "learning_rate": 0.0003314289795043919,
      "loss": 0.4635,
      "step": 10100
    },
    {
      "epoch": 0.6886024423337856,
      "grad_norm": 0.8935447931289673,
      "learning_rate": 0.0003278583160751268,
      "loss": 0.4908,
      "step": 10150
    },
    {
      "epoch": 0.6919945725915875,
      "grad_norm": 0.7473117113113403,
      "learning_rate": 0.00032428765264586165,
      "loss": 0.4634,
      "step": 10200
    },
    {
      "epoch": 0.6953867028493894,
      "grad_norm": 0.7751464247703552,
      "learning_rate": 0.00032071698921659646,
      "loss": 0.4609,
      "step": 10250
    },
    {
      "epoch": 0.6987788331071914,
      "grad_norm": 0.6482067108154297,
      "learning_rate": 0.0003171463257873313,
      "loss": 0.4504,
      "step": 10300
    },
    {
      "epoch": 0.7021709633649932,
      "grad_norm": 1.214855670928955,
      "learning_rate": 0.0003135756623580661,
      "loss": 0.481,
      "step": 10350
    },
    {
      "epoch": 0.7055630936227951,
      "grad_norm": 1.015939474105835,
      "learning_rate": 0.00031000499892880096,
      "loss": 0.4807,
      "step": 10400
    },
    {
      "epoch": 0.7089552238805971,
      "grad_norm": 1.0234791040420532,
      "learning_rate": 0.00030643433549953583,
      "loss": 0.4664,
      "step": 10450
    },
    {
      "epoch": 0.7123473541383989,
      "grad_norm": 0.9963476061820984,
      "learning_rate": 0.00030286367207027064,
      "loss": 0.4962,
      "step": 10500
    },
    {
      "epoch": 0.7157394843962008,
      "grad_norm": 0.8159264326095581,
      "learning_rate": 0.0002992930086410055,
      "loss": 0.4429,
      "step": 10550
    },
    {
      "epoch": 0.7191316146540027,
      "grad_norm": 0.5652539730072021,
      "learning_rate": 0.0002957223452117404,
      "loss": 0.4541,
      "step": 10600
    },
    {
      "epoch": 0.7225237449118046,
      "grad_norm": 1.2022106647491455,
      "learning_rate": 0.0002921516817824752,
      "loss": 0.4615,
      "step": 10650
    },
    {
      "epoch": 0.7259158751696065,
      "grad_norm": 0.7013974189758301,
      "learning_rate": 0.00028858101835321006,
      "loss": 0.4439,
      "step": 10700
    },
    {
      "epoch": 0.7293080054274084,
      "grad_norm": 0.6197811365127563,
      "learning_rate": 0.00028501035492394487,
      "loss": 0.4618,
      "step": 10750
    },
    {
      "epoch": 0.7327001356852103,
      "grad_norm": 0.6442908048629761,
      "learning_rate": 0.00028143969149467974,
      "loss": 0.4737,
      "step": 10800
    },
    {
      "epoch": 0.7360922659430122,
      "grad_norm": 0.9292165637016296,
      "learning_rate": 0.0002778690280654146,
      "loss": 0.435,
      "step": 10850
    },
    {
      "epoch": 0.7394843962008141,
      "grad_norm": 0.6415395140647888,
      "learning_rate": 0.00027429836463614937,
      "loss": 0.4717,
      "step": 10900
    },
    {
      "epoch": 0.742876526458616,
      "grad_norm": 0.8680168390274048,
      "learning_rate": 0.00027072770120688424,
      "loss": 0.4705,
      "step": 10950
    },
    {
      "epoch": 0.746268656716418,
      "grad_norm": 0.7258002758026123,
      "learning_rate": 0.00026715703777761905,
      "loss": 0.4762,
      "step": 11000
    },
    {
      "epoch": 0.7496607869742198,
      "grad_norm": 0.9515918493270874,
      "learning_rate": 0.0002635863743483539,
      "loss": 0.4598,
      "step": 11050
    },
    {
      "epoch": 0.7530529172320217,
      "grad_norm": 0.834819495677948,
      "learning_rate": 0.0002600157109190888,
      "loss": 0.4433,
      "step": 11100
    },
    {
      "epoch": 0.7564450474898237,
      "grad_norm": 0.8420964479446411,
      "learning_rate": 0.0002564450474898236,
      "loss": 0.4413,
      "step": 11150
    },
    {
      "epoch": 0.7598371777476255,
      "grad_norm": 0.6821955442428589,
      "learning_rate": 0.00025287438406055847,
      "loss": 0.452,
      "step": 11200
    },
    {
      "epoch": 0.7632293080054274,
      "grad_norm": 0.8335272073745728,
      "learning_rate": 0.0002493037206312933,
      "loss": 0.4722,
      "step": 11250
    },
    {
      "epoch": 0.7666214382632293,
      "grad_norm": 1.0254689455032349,
      "learning_rate": 0.00024573305720202815,
      "loss": 0.4466,
      "step": 11300
    },
    {
      "epoch": 0.7700135685210312,
      "grad_norm": 0.666927695274353,
      "learning_rate": 0.00024216239377276296,
      "loss": 0.4554,
      "step": 11350
    },
    {
      "epoch": 0.7734056987788331,
      "grad_norm": 0.8175921440124512,
      "learning_rate": 0.00023859173034349783,
      "loss": 0.4522,
      "step": 11400
    },
    {
      "epoch": 0.776797829036635,
      "grad_norm": 0.8455876111984253,
      "learning_rate": 0.00023502106691423267,
      "loss": 0.4551,
      "step": 11450
    },
    {
      "epoch": 0.7801899592944369,
      "grad_norm": 0.7475613355636597,
      "learning_rate": 0.0002314504034849675,
      "loss": 0.446,
      "step": 11500
    },
    {
      "epoch": 0.7835820895522388,
      "grad_norm": 0.9018256068229675,
      "learning_rate": 0.00022787974005570235,
      "loss": 0.4303,
      "step": 11550
    },
    {
      "epoch": 0.7869742198100407,
      "grad_norm": 0.9121310114860535,
      "learning_rate": 0.00022430907662643722,
      "loss": 0.4234,
      "step": 11600
    },
    {
      "epoch": 0.7903663500678426,
      "grad_norm": 0.7824239730834961,
      "learning_rate": 0.00022073841319717203,
      "loss": 0.4455,
      "step": 11650
    },
    {
      "epoch": 0.7937584803256446,
      "grad_norm": 0.8161771893501282,
      "learning_rate": 0.00021716774976790688,
      "loss": 0.4363,
      "step": 11700
    },
    {
      "epoch": 0.7971506105834464,
      "grad_norm": 0.8142558336257935,
      "learning_rate": 0.00021359708633864172,
      "loss": 0.4224,
      "step": 11750
    },
    {
      "epoch": 0.8005427408412483,
      "grad_norm": 0.9621278047561646,
      "learning_rate": 0.00021002642290937656,
      "loss": 0.4506,
      "step": 11800
    },
    {
      "epoch": 0.8039348710990502,
      "grad_norm": 0.9135773777961731,
      "learning_rate": 0.00020645575948011142,
      "loss": 0.4232,
      "step": 11850
    },
    {
      "epoch": 0.8073270013568521,
      "grad_norm": 0.9209827780723572,
      "learning_rate": 0.00020288509605084627,
      "loss": 0.4324,
      "step": 11900
    },
    {
      "epoch": 0.810719131614654,
      "grad_norm": 1.0143301486968994,
      "learning_rate": 0.00019931443262158108,
      "loss": 0.4328,
      "step": 11950
    },
    {
      "epoch": 0.8141112618724559,
      "grad_norm": 0.8586130142211914,
      "learning_rate": 0.00019574376919231592,
      "loss": 0.4349,
      "step": 12000
    },
    {
      "epoch": 0.8175033921302578,
      "grad_norm": 1.0349270105361938,
      "learning_rate": 0.0001921731057630508,
      "loss": 0.4367,
      "step": 12050
    },
    {
      "epoch": 0.8208955223880597,
      "grad_norm": 0.6939113736152649,
      "learning_rate": 0.00018860244233378563,
      "loss": 0.4162,
      "step": 12100
    },
    {
      "epoch": 0.8242876526458616,
      "grad_norm": 0.7723024487495422,
      "learning_rate": 0.00018503177890452047,
      "loss": 0.4618,
      "step": 12150
    },
    {
      "epoch": 0.8276797829036635,
      "grad_norm": 0.6600919365882874,
      "learning_rate": 0.0001814611154752553,
      "loss": 0.3983,
      "step": 12200
    },
    {
      "epoch": 0.8310719131614654,
      "grad_norm": 0.6771370768547058,
      "learning_rate": 0.00017789045204599012,
      "loss": 0.4462,
      "step": 12250
    },
    {
      "epoch": 0.8344640434192673,
      "grad_norm": 0.6874603033065796,
      "learning_rate": 0.000174319788616725,
      "loss": 0.443,
      "step": 12300
    },
    {
      "epoch": 0.8378561736770692,
      "grad_norm": 0.9171736240386963,
      "learning_rate": 0.00017074912518745983,
      "loss": 0.4421,
      "step": 12350
    },
    {
      "epoch": 0.841248303934871,
      "grad_norm": 0.7990365624427795,
      "learning_rate": 0.00016717846175819467,
      "loss": 0.4273,
      "step": 12400
    },
    {
      "epoch": 0.844640434192673,
      "grad_norm": 0.725435733795166,
      "learning_rate": 0.00016360779832892951,
      "loss": 0.4058,
      "step": 12450
    },
    {
      "epoch": 0.8480325644504749,
      "grad_norm": 0.6998252272605896,
      "learning_rate": 0.00016003713489966438,
      "loss": 0.4368,
      "step": 12500
    },
    {
      "epoch": 0.8514246947082768,
      "grad_norm": 0.8491682410240173,
      "learning_rate": 0.0001564664714703992,
      "loss": 0.4478,
      "step": 12550
    },
    {
      "epoch": 0.8548168249660787,
      "grad_norm": 0.8313434720039368,
      "learning_rate": 0.00015289580804113404,
      "loss": 0.4344,
      "step": 12600
    },
    {
      "epoch": 0.8582089552238806,
      "grad_norm": 0.7010881900787354,
      "learning_rate": 0.00014932514461186888,
      "loss": 0.4502,
      "step": 12650
    },
    {
      "epoch": 0.8616010854816825,
      "grad_norm": 0.8265959620475769,
      "learning_rate": 0.00014575448118260375,
      "loss": 0.4394,
      "step": 12700
    },
    {
      "epoch": 0.8649932157394844,
      "grad_norm": 0.6438841223716736,
      "learning_rate": 0.00014218381775333859,
      "loss": 0.4296,
      "step": 12750
    },
    {
      "epoch": 0.8683853459972863,
      "grad_norm": 0.6672186851501465,
      "learning_rate": 0.00013861315432407343,
      "loss": 0.4244,
      "step": 12800
    },
    {
      "epoch": 0.8717774762550882,
      "grad_norm": 0.8150332570075989,
      "learning_rate": 0.00013504249089480824,
      "loss": 0.4495,
      "step": 12850
    },
    {
      "epoch": 0.8751696065128901,
      "grad_norm": 0.8075096607208252,
      "learning_rate": 0.00013147182746554308,
      "loss": 0.423,
      "step": 12900
    },
    {
      "epoch": 0.878561736770692,
      "grad_norm": 1.0046402215957642,
      "learning_rate": 0.00012790116403627795,
      "loss": 0.4416,
      "step": 12950
    },
    {
      "epoch": 0.8819538670284939,
      "grad_norm": 0.8503800630569458,
      "learning_rate": 0.0001243305006070128,
      "loss": 0.433,
      "step": 13000
    },
    {
      "epoch": 0.8853459972862958,
      "grad_norm": 0.70710289478302,
      "learning_rate": 0.00012075983717774763,
      "loss": 0.4127,
      "step": 13050
    },
    {
      "epoch": 0.8887381275440976,
      "grad_norm": 0.6947090029716492,
      "learning_rate": 0.00011718917374848247,
      "loss": 0.4326,
      "step": 13100
    },
    {
      "epoch": 0.8921302578018996,
      "grad_norm": 0.7625718116760254,
      "learning_rate": 0.00011361851031921731,
      "loss": 0.4311,
      "step": 13150
    },
    {
      "epoch": 0.8955223880597015,
      "grad_norm": 0.7305395603179932,
      "learning_rate": 0.00011004784688995217,
      "loss": 0.4341,
      "step": 13200
    },
    {
      "epoch": 0.8989145183175034,
      "grad_norm": 1.0260063409805298,
      "learning_rate": 0.000106477183460687,
      "loss": 0.4309,
      "step": 13250
    },
    {
      "epoch": 0.9023066485753053,
      "grad_norm": 0.8722951412200928,
      "learning_rate": 0.00010290652003142184,
      "loss": 0.4117,
      "step": 13300
    },
    {
      "epoch": 0.9056987788331072,
      "grad_norm": 0.7847393155097961,
      "learning_rate": 9.933585660215669e-05,
      "loss": 0.4445,
      "step": 13350
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.6998234987258911,
      "learning_rate": 9.576519317289152e-05,
      "loss": 0.4323,
      "step": 13400
    },
    {
      "epoch": 0.912483039348711,
      "grad_norm": 0.7197815179824829,
      "learning_rate": 9.219452974362637e-05,
      "loss": 0.4268,
      "step": 13450
    },
    {
      "epoch": 0.9158751696065129,
      "grad_norm": 0.6085216999053955,
      "learning_rate": 8.862386631436121e-05,
      "loss": 0.4207,
      "step": 13500
    },
    {
      "epoch": 0.9192672998643148,
      "grad_norm": 0.9235864281654358,
      "learning_rate": 8.505320288509605e-05,
      "loss": 0.4013,
      "step": 13550
    },
    {
      "epoch": 0.9226594301221167,
      "grad_norm": 1.0848759412765503,
      "learning_rate": 8.14825394558309e-05,
      "loss": 0.4123,
      "step": 13600
    },
    {
      "epoch": 0.9260515603799185,
      "grad_norm": 0.8368447422981262,
      "learning_rate": 7.791187602656575e-05,
      "loss": 0.4148,
      "step": 13650
    },
    {
      "epoch": 0.9294436906377205,
      "grad_norm": 1.0053642988204956,
      "learning_rate": 7.434121259730058e-05,
      "loss": 0.4166,
      "step": 13700
    },
    {
      "epoch": 0.9328358208955224,
      "grad_norm": 0.6009365916252136,
      "learning_rate": 7.077054916803542e-05,
      "loss": 0.4038,
      "step": 13750
    },
    {
      "epoch": 0.9362279511533242,
      "grad_norm": 0.8309950828552246,
      "learning_rate": 6.719988573877027e-05,
      "loss": 0.434,
      "step": 13800
    },
    {
      "epoch": 0.9396200814111262,
      "grad_norm": 0.9272484183311462,
      "learning_rate": 6.36292223095051e-05,
      "loss": 0.4005,
      "step": 13850
    },
    {
      "epoch": 0.9430122116689281,
      "grad_norm": 0.6062020063400269,
      "learning_rate": 6.005855888023995e-05,
      "loss": 0.4004,
      "step": 13900
    },
    {
      "epoch": 0.94640434192673,
      "grad_norm": 0.893446683883667,
      "learning_rate": 5.648789545097479e-05,
      "loss": 0.4215,
      "step": 13950
    },
    {
      "epoch": 0.9497964721845319,
      "grad_norm": 0.8847548365592957,
      "learning_rate": 5.2917232021709634e-05,
      "loss": 0.435,
      "step": 14000
    },
    {
      "epoch": 0.9531886024423338,
      "grad_norm": 0.8607507348060608,
      "learning_rate": 4.9346568592444474e-05,
      "loss": 0.4116,
      "step": 14050
    },
    {
      "epoch": 0.9565807327001357,
      "grad_norm": 0.7247753143310547,
      "learning_rate": 4.5775905163179315e-05,
      "loss": 0.4047,
      "step": 14100
    },
    {
      "epoch": 0.9599728629579376,
      "grad_norm": 0.566663920879364,
      "learning_rate": 4.220524173391416e-05,
      "loss": 0.416,
      "step": 14150
    },
    {
      "epoch": 0.9633649932157394,
      "grad_norm": 0.793378472328186,
      "learning_rate": 3.8634578304649004e-05,
      "loss": 0.3908,
      "step": 14200
    },
    {
      "epoch": 0.9667571234735414,
      "grad_norm": 1.175868272781372,
      "learning_rate": 3.5063914875383845e-05,
      "loss": 0.4119,
      "step": 14250
    },
    {
      "epoch": 0.9701492537313433,
      "grad_norm": 0.979056715965271,
      "learning_rate": 3.149325144611869e-05,
      "loss": 0.4003,
      "step": 14300
    },
    {
      "epoch": 0.9735413839891451,
      "grad_norm": 0.8199784755706787,
      "learning_rate": 2.792258801685353e-05,
      "loss": 0.4004,
      "step": 14350
    },
    {
      "epoch": 0.9769335142469471,
      "grad_norm": 0.7732757925987244,
      "learning_rate": 2.4351924587588374e-05,
      "loss": 0.4213,
      "step": 14400
    },
    {
      "epoch": 0.980325644504749,
      "grad_norm": 0.9394141435623169,
      "learning_rate": 2.0781261158323218e-05,
      "loss": 0.4097,
      "step": 14450
    },
    {
      "epoch": 0.9837177747625508,
      "grad_norm": 0.6240217089653015,
      "learning_rate": 1.721059772905806e-05,
      "loss": 0.4316,
      "step": 14500
    },
    {
      "epoch": 0.9871099050203528,
      "grad_norm": 0.6001991033554077,
      "learning_rate": 1.3639934299792901e-05,
      "loss": 0.4039,
      "step": 14550
    },
    {
      "epoch": 0.9905020352781547,
      "grad_norm": 1.0915993452072144,
      "learning_rate": 1.0069270870527744e-05,
      "loss": 0.393,
      "step": 14600
    },
    {
      "epoch": 0.9938941655359566,
      "grad_norm": 0.673319935798645,
      "learning_rate": 6.498607441262586e-06,
      "loss": 0.4204,
      "step": 14650
    },
    {
      "epoch": 0.9972862957937585,
      "grad_norm": 0.6997135877609253,
      "learning_rate": 2.927944011997429e-06,
      "loss": 0.4171,
      "step": 14700
    }
  ],
  "logging_steps": 50,
  "max_steps": 14740,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.17167081807718e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
